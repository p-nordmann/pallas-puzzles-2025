{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11d21928",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-nordmann/pallas-puzzles-2025/blob/main/notebooks/Pallas-Puzzles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05199f6b-13f0-49af-9a57-d4e98f80ef47",
      "metadata": {
        "id": "05199f6b-13f0-49af-9a57-d4e98f80ef47"
      },
      "source": [
        "# Let's solve some puzzles!\n",
        "\n",
        "Programming for accelerators such as GPUs is critical for modern AI systems. Pallas is a JAX-integrated [Triton](https://github.com/openai/triton/)-like language. Triton is an alternative open-source language that allows you to code at a higher-level and compile to accelerators like GPU.\n",
        "\n",
        "<p align=\"left\"><img alt=\"illustration\" src=\"../assets/triton_load.png\" width=\"50%\"/></p>\n",
        "\n",
        "Coding for Pallas is very similar to Numpy and JAX in both syntax and semantics. However, as a lower-level language there are a lot of details that you need to keep track of. In particular, one area that learners have trouble with is memory loading and storage which is critical for speed on low-level devices.\n",
        "\n",
        "This set is puzzles is meant to teach you how to use Pallas from first principles in an interactive fashion. You will start with trivial examples and build your way up to real algorithms like Flash Attention and Quantized neural networks. These puzzles **do not** need to run on GPU since Pallas has an interpreter, but be careful as the interperter might produce somewhat different results to the GPU with masked loads and stores -- this'll hopefully be fixed soon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a224fbf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Only need to run the first time.\n",
        "!pip install jaxtyping\n",
        "!pip install \"jax[cuda12]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aN1sj9TrMifV",
      "metadata": {
        "id": "aN1sj9TrMifV"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax import Array\n",
        "from jax import numpy as jnp\n",
        "from jax.experimental import pallas as pl\n",
        "from jaxtyping import Float32, Int32\n",
        "\n",
        "from utils import test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ff711c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# We set an INTERPRET environment variable.\n",
        "# If you have a GPU available and want to run the puzzles on it, set it to True.\n",
        "interpret = True\n",
        "\n",
        "os.environ[\"INTERPRET\"] = str(interpret)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RyFiT-KGDxqQ",
      "metadata": {
        "id": "RyFiT-KGDxqQ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "To begin with, we will only use `pl.load` and `pl.store` in order to build simple programs.\n",
        "\n",
        "Here's an example of load. It takes an `arange` over the memory. By default the indexing of JAX Arrays with column, rows, depths or right-to-left. It also takes in a mask as an optional argument. The load/store mask is critically important because all shapes in Triton need to be powers of two.\n",
        "\n",
        "We'll start with a program that both reads (`pl.load`) and writes (`pl.store`) to memory. The reason we cannot just read from memory is that JAX will skip calling a function that does not have a side effect. This is by design.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd973910",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pallas_demo(x_ref, y_ref):\n",
        "    # a mask is used to not read/write data out of bounds\n",
        "    # x_mask = jnp.arange(x_ref.size) < 5\n",
        "    x_mask = jnp.arange(x_ref.size) < 5\n",
        "    # let's read only 5 elements, the rest will be 0\n",
        "    x_index = (pl.dslice(0, x_ref.shape[0]),)  # slice dimensions,\n",
        "    # `[pl.dslice(None)]` is equivalent to Python's `:`\n",
        "    x = pl.load(x_ref, x_index, mask=x_mask, other=0)\n",
        "\n",
        "    # when executing in a debugging mode (`interpret=True`) you can use\n",
        "    # `jax.debug.print` to print values.\n",
        "    # jax.debug.print(\"x = {}\", x)\n",
        "\n",
        "    # pl.store takes (reference/pointer, slice dimensions, data)\n",
        "    y_mask = jnp.arange(y_ref.size) < 6  # let's write only 6 elements\n",
        "\n",
        "    # we can also specify a slice as `pl.dslice(start, size)`\n",
        "    # however, note that size has to be available at compile time, start does not\n",
        "    # [pl.dslice(0, 6)] is equivalent to Python's y_ref[0:]\n",
        "    y_index = (pl.dslice(0, y_ref.shape[0]),)\n",
        "    pl.store(y_ref, y_index, (x + 1), mask=y_mask)\n",
        "\n",
        "\n",
        "# example data\n",
        "x = jnp.arange(8).astype(jnp.float32)\n",
        "\n",
        "# input spec, how to split input on the grid\n",
        "# BlockSpec takes two args:\n",
        "# - grid dimension lambda that outputs start idx\n",
        "# - size of data reference to pass to each program (this is copy-free)\n",
        "in_spec = pl.BlockSpec(x.shape, lambda i: (0,))\n",
        "\n",
        "y_shape = x.shape\n",
        "out_spec = pl.BlockSpec(y_shape, lambda i: (0,))\n",
        "\n",
        "out = pl.pallas_call(\n",
        "    pallas_demo,\n",
        "    grid=(1,),\n",
        "    in_specs=[in_spec],  # input spec must be a list/tuple\n",
        "    out_specs=out_spec,  # output spec for Pallas, how to split output\n",
        "    out_shape=jax.ShapeDtypeStruct(y_shape, jnp.float32),  # output shape for JIT\n",
        "    interpret=True,  # execute on CPU, debug statements do not work on GPU\n",
        ")(x)\n",
        "print(f\"out = {out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W9appXLw4Bka",
      "metadata": {
        "id": "W9appXLw4Bka"
      },
      "source": [
        "You can also use this trick to read in a 2d array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_981RFRp4Avz",
      "metadata": {
        "id": "_981RFRp4Avz"
      },
      "outputs": [],
      "source": [
        "def pallas_demo(x_ref, y_ref):\n",
        "    # index for 2 axis/dims `[:, :]`\n",
        "    x_index = (pl.dslice(None), pl.dslice(None))\n",
        "    x = pl.load(x_ref, x_index)\n",
        "    jax.debug.print(\"x =\\n{}\", x)\n",
        "\n",
        "    y_index = (pl.dslice(None), pl.dslice(None))\n",
        "    # two dimensional mask using numpy-like broadcasting\n",
        "    # with `&` (logical and) operator\n",
        "    y_mask = (jnp.arange(y_ref.shape[0]) < 2)[:, None]\n",
        "    y_mask &= (jnp.arange(y_ref.shape[1]) < 3)[None, :]\n",
        "    pl.store(y_ref, y_index, (x + 1) ** 2, mask=y_mask)\n",
        "\n",
        "\n",
        "# example data\n",
        "x = jnp.arange(4**2).reshape((4, 4)).astype(jnp.float32)\n",
        "\n",
        "out = pl.pallas_call(\n",
        "    pallas_demo,\n",
        "    grid=(1,),\n",
        "    in_specs=[pl.BlockSpec(x.shape, lambda i: (0, 0))],\n",
        "    out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)),\n",
        "    out_shape=jax.ShapeDtypeStruct(x.shape, jnp.float32),\n",
        "    interpret=True,\n",
        ")(x)\n",
        "print(f\"out =\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E20TwfudJzyq",
      "metadata": {
        "id": "E20TwfudJzyq"
      },
      "source": [
        "You can only load in relatively small `blocks` at a time in Pallas. to work with larger tensors you need to use a program id axis to run multiple blocks in parallel. Here is an example with one program axis with 3 blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e756f858",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pallas_demo(x_ref, y_ref):\n",
        "    pid = pl.program_id(0)\n",
        "\n",
        "    # index for 2 axis/dims `[2*i:2*i+2, 2*i:2*i+1]`\n",
        "    x_index = (pl.dslice(2 * pid, 2), pl.dslice(2 * pid, 2))\n",
        "    x = pl.load(x_ref, x_index)\n",
        "    jax.debug.print(\"x =\\n{}\", x)\n",
        "\n",
        "    # index for 2 axis/dims `[2*i:2*i+2, 2*i:2*i+1]`\n",
        "    y_index = (pl.dslice(2 * pid, 2), pl.dslice(2 * pid, 2))\n",
        "    pl.store(y_ref, y_index, (x + 1) ** 2)\n",
        "\n",
        "\n",
        "# example data\n",
        "x = jnp.arange(4**2).reshape((4, 4)).astype(jnp.float32)\n",
        "\n",
        "# this kernel call is not great, we're only writing to block diagonal elements\n",
        "# of y; the rest is uninitialized memory\n",
        "out = pl.pallas_call(\n",
        "    pallas_demo,\n",
        "    grid=(2,),  # NOTE: we changed the grid size\n",
        "    in_specs=[pl.BlockSpec(x.shape, lambda i: (0, 0))],\n",
        "    out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)),\n",
        "    out_shape=jax.ShapeDtypeStruct(x.shape, jnp.float32),\n",
        "    interpret=True,\n",
        ")(x)\n",
        "print(f\"out =\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xynrKqqSKYi-",
      "metadata": {
        "id": "xynrKqqSKYi-"
      },
      "source": [
        "See the [Pallas Docs](https://jax.readthedocs.io/en/latest/pallas/index.html) for further information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "058b7a6b",
      "metadata": {},
      "source": [
        "## Prerequisites: grid and BlockSpec\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f617b944-cf73-4bad-b7d6-1cc6d17879a9",
      "metadata": {
        "id": "f617b944-cf73-4bad-b7d6-1cc6d17879a9"
      },
      "source": [
        "## Puzzle 1: Constant Add\n",
        "\n",
        "Add a constant to a vector. Uses one program id axis. Block size `B0` is always the same as vector `x` with length `N0`.\n",
        "\n",
        "$$z_i = 10 + x_i \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/1_constant_add.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5fdae5-156e-48e6-abb3-9abe9d258229",
      "metadata": {
        "id": "3d5fdae5-156e-48e6-abb3-9abe9d258229"
      },
      "outputs": [],
      "source": [
        "N0 = 32\n",
        "\n",
        "\n",
        "def add_spec(x: Float32[Array, f\"{N0}\"]) -> Float32[Array, f\"{N0}\"]:\n",
        "    \"This is the spec that you should implement. Uses typing to define sizes.\"\n",
        "    return x + 10.0\n",
        "\n",
        "\n",
        "def add_kernel(x_ref, z_ref, B0: int):\n",
        "    pass\n",
        "    # Finish me!\n",
        "\n",
        "\n",
        "test(add_kernel, add_spec, nelem={\"N0\": N0})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863aa741-98ba-4370-970a-7378a075fadb",
      "metadata": {
        "id": "863aa741-98ba-4370-970a-7378a075fadb"
      },
      "source": [
        "## Puzzle 2: Constant Add Block\n",
        "\n",
        "Add a constant to a vector. Uses one program block axis (no `for` loops yet). Block size `B0` is now smaller than the shape vector `x` which is `N0`.\n",
        "\n",
        "$$z_i = 10 + x_i \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/2_constant_add_block.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74baeaed-82d0-4814-b1fc-e9a51237cf7e",
      "metadata": {
        "id": "74baeaed-82d0-4814-b1fc-e9a51237cf7e"
      },
      "outputs": [],
      "source": [
        "N0 = 100\n",
        "\n",
        "\n",
        "def add2_spec(x: Float32[Array, f\"{N0}\"]) -> Float32[Array, f\"{N0}\"]:\n",
        "    return x + 10.0\n",
        "\n",
        "\n",
        "def add_mask2_kernel(x_ref, z_ref, B0: int):\n",
        "    pid = pl.program_id(0)\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(add_mask2_kernel, add2_spec, nelem={\"N0\": N0})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "639dbf91-18b1-4bc6-8624-609b1031e9e1",
      "metadata": {
        "id": "639dbf91-18b1-4bc6-8624-609b1031e9e1"
      },
      "source": [
        "## Puzzle 3: Outer Vector Add\n",
        "\n",
        "Add two vectors.\n",
        "\n",
        "Uses one program block axis. Block size `B0` is always the same as vector `x` length `N0`.\n",
        "Block size `B1` is always the same as vector `y` length `N1`.\n",
        "\n",
        "$$z_{j, i} = x_i + y_j\\text{ for } i = 1\\ldots B_0,\\ j = 1\\ldots B_1$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/3_outer_vector_add.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a977409-3fcb-4b72-abee-f09d02bf6f70",
      "metadata": {
        "id": "5a977409-3fcb-4b72-abee-f09d02bf6f70"
      },
      "outputs": [],
      "source": [
        "N0, N1 = 32, 32\n",
        "\n",
        "\n",
        "def add_vec_spec(\n",
        "    x: Float32[Array, f\"{N0}\"], y: Float32[Array, f\"{N1}\"]\n",
        ") -> Float32[Array, f\"{N1} {N0}\"]:\n",
        "    return x[None, :] + y[:, None]\n",
        "\n",
        "\n",
        "def add_vec_kernel(x_ref, y_ref, z_ref, B0: int, B1: int):\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(add_vec_kernel, add_vec_spec, nelem={\"N0\": N0, \"N1\": N1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea0bd06-58f5-4088-af2a-f3c0d80f5e15",
      "metadata": {
        "id": "1ea0bd06-58f5-4088-af2a-f3c0d80f5e15"
      },
      "source": [
        "## Puzzle 4: Outer Vector Add Block\n",
        "\n",
        "Add a row vector to a column vector.\n",
        "\n",
        "Uses two program block axes. Block size `B0` is always less than the vector `x` length `N0`.\n",
        "Block size `B1` is always less than vector `y` length `N1`.\n",
        "\n",
        "$$z_{j, i} = x_i + y_j\\text{ for } i = 1\\ldots N_0,\\ j = 1\\ldots N_1$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/4_outer_vector_add_block.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbbe277-344b-4750-b9aa-f7b7da6303d4",
      "metadata": {
        "id": "4dbbe277-344b-4750-b9aa-f7b7da6303d4"
      },
      "outputs": [],
      "source": [
        "N0, N1 = 100, 90\n",
        "\n",
        "\n",
        "def add_vec_block_spec(\n",
        "    x: Float32[Array, f\"{N0}\"], y: Float32[Array, f\"{N1}\"]\n",
        ") -> Float32[Array, f\"{N0} {N1}\"]:\n",
        "    return x[None, :] + y[:, None]\n",
        "\n",
        "\n",
        "def add_vec_block_kernel(x_ref, y_ref, z_ref, B0: int, B1: int):\n",
        "    pid_i, pid_j = pl.program_id(0), pl.program_id(1)\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(add_vec_block_kernel, add_vec_block_spec, nelem={\"N0\": N0, \"N1\": N1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025a1cfa-6b4e-4456-acc4-d15aa35ec605",
      "metadata": {
        "id": "025a1cfa-6b4e-4456-acc4-d15aa35ec605"
      },
      "source": [
        "## Puzzle 5: Fused Outer Multiplication\n",
        "\n",
        "Multiply a row vector to a column vector and take a relu.\n",
        "\n",
        "Uses two program block axes. Block size `B0` is always less than the vector `x` length `N0`.\n",
        "Block size `B1` is always less than vector `y` length `N1`.\n",
        "\n",
        "$$z_{j, i} = \\text{relu}(x_i \\times y_j)\\text{ for } i = 1\\ldots N_0,\\ j = 1\\ldots N_1$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/5_fused_outer_multiplication.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55955d0-b670-4dc4-9db1-5f28ce4f56fb",
      "metadata": {
        "id": "b55955d0-b670-4dc4-9db1-5f28ce4f56fb"
      },
      "outputs": [],
      "source": [
        "N0, N1 = 100, 90\n",
        "\n",
        "\n",
        "def mul_relu_block_spec(\n",
        "    x: Float32[Array, f\"{N0}\"], y: Float32[Array, f\"{N1}\"]\n",
        ") -> Float32[Array, f\"{N1} {N0}\"]:\n",
        "    return jax.nn.relu(x[None, :] * y[:, None])\n",
        "\n",
        "\n",
        "def mul_relu_block_kernel(x_ref, y_ref, z_ref, B0: int, B1: int):\n",
        "    pid_i, pid_j = pl.program_id(0), pl.program_id(1)\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(mul_relu_block_kernel, mul_relu_block_spec, nelem={\"N0\": N0, \"N1\": N1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1677c8-a1e8-476c-bf04-8a6ba912c84d",
      "metadata": {
        "id": "9e1677c8-a1e8-476c-bf04-8a6ba912c84d"
      },
      "source": [
        "## Puzzle 6: Fused Outer Multiplication - Backwards\n",
        "\n",
        "Backwards of a function that multiplies a matrix with a row vector and take a relu.\n",
        "\n",
        "Uses two program blocks. Block size `B0` is always less than the vector `x` length `N0`.\n",
        "Block size `B1` is always less than vector `y` length `N1`. Chain rule backward `dz`\n",
        "is of shape `N1` by `N0`\n",
        "\n",
        "$$f(x, y) = \\text{relu}(x_i \\times y_j)\\text{ for } i = 1\\ldots N_0,\\ j = 1\\ldots N_1$$\n",
        "\n",
        "$$dx_{i, j} = f_x'(x, y)_{i, j} \\times dz_{i,j}$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/6_fused_outer_multiplication_backwards.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f06cbdd-1c9b-4b65-ae72-69c1dc599400",
      "metadata": {
        "id": "9f06cbdd-1c9b-4b65-ae72-69c1dc599400"
      },
      "outputs": [],
      "source": [
        "N0, N1 = 100, 90\n",
        "\n",
        "\n",
        "def mul_relu_block_back_spec(\n",
        "    x: Float32[Array, f\"{N1} {N0}\"],\n",
        "    y: Float32[Array, f\"{N1}\"],\n",
        "    dz: Float32[Array, f\"{N1} {N0}\"],\n",
        ") -> Float32[Array, f\"{N1} {N0}\"]:\n",
        "    return jax.grad(lambda x, y: jnp.sum(jax.nn.relu(x * y[:, None]) * dz))(x, y)\n",
        "\n",
        "\n",
        "def mul_relu_block_back_kernel(x_ref, y_ref, dz_ref, dx_ref, B0: int, B1: int):\n",
        "    pid_i, pid_j = pl.program_id(0), pl.program_id(1)\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(mul_relu_block_back_kernel, mul_relu_block_back_spec, nelem={\"N0\": N0, \"N1\": N1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GQjO0gHj_5Sr",
      "metadata": {
        "id": "GQjO0gHj_5Sr"
      },
      "source": [
        "## Puzzle 7: Long Sum\n",
        "\n",
        "Sum of a batch of numbers.\n",
        "\n",
        "Uses one program blocks. Block size `B0` represents a range of batches of `x` of length `N0`.\n",
        "Each element is of length `T`. Process it `B1 < T` elements at a time.\n",
        "\n",
        "$$z_{i} = \\sum^{T}_j x_{i,j} =  \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/7_long_sum.png\" /></p>\n",
        "\n",
        "Hint: You will need a for loop for this problem. These work and look the same as in Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2zr8atR5_5Sr",
      "metadata": {
        "id": "2zr8atR5_5Sr"
      },
      "outputs": [],
      "source": [
        "N0, N1 = 4, 200\n",
        "\n",
        "\n",
        "def sum_spec(x: Float32[Array, f\"{N0} {N1}\"]) -> Float32[Array, f\"{N0}\"]:\n",
        "    return jnp.sum(x, -1)\n",
        "\n",
        "\n",
        "def sum_kernel(x_ref, z_ref, B0: int, B1: int):\n",
        "    pid_i = pl.program_id(0)\n",
        "    T = x_ref.shape[-1]\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(sum_kernel, sum_spec, B={\"B0\": 1, \"B1\": 32}, nelem={\"N0\": N0, \"N1\": N1, \"T\": 200})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86402b17-9fa4-489b-8c79-71d4ecb68ff2",
      "metadata": {
        "id": "86402b17-9fa4-489b-8c79-71d4ecb68ff2"
      },
      "source": [
        "## Puzzle 8: Long Softmax\n",
        "\n",
        "Softmax of a batch of logits.\n",
        "\n",
        "Uses one program block axis. Block size `B0` represents the batch of `x` of length `N0`.\n",
        "Block logit length `T`. Process it `B1 < T` elements at a time.\n",
        "\n",
        "$$z_{i, j} = \\text{softmax}(x_{i,1} \\ldots x_{i, T}) \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "Note softmax needs to be computed in numerically stable form as in Python. In addition in Triton they recommend not using `exp` but instead using `exp2`. You need the identity\n",
        "\n",
        "$$\\exp(x) = 2^{\\log_2(e) x}$$\n",
        "\n",
        "Advanced: there one way to do this with 3 loops. You can also do it with 2 loops if you are clever. Hint: you will find this identity useful:\n",
        "\n",
        "$$\\exp(x_i - m) =  \\exp(x_i - m/2 - m/2) = \\exp(x_i - m/ 2) /  \\exp(m/2) $$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/8_long_softmax.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9a50fd-3774-420c-9fec-faac8fdcc7de",
      "metadata": {
        "id": "2c9a50fd-3774-420c-9fec-faac8fdcc7de"
      },
      "outputs": [],
      "source": [
        "N0, N1 = 4, 200\n",
        "\n",
        "\n",
        "def softmax_spec(x: Float32[Array, f\"{N0} {N1}\"]) -> Float32[Array, f\"{N0} {N1}\"]:\n",
        "    x_max = jnp.max(x, -1)[..., None]\n",
        "    x = x - x_max\n",
        "    x_exp = jnp.exp(x)\n",
        "    return x_exp / jnp.sum(x_exp, -1)[..., None]\n",
        "\n",
        "\n",
        "def softmax_kernel(x_ref, z_ref, B0: int, B1: int) -> None:\n",
        "    pid_i = pl.program_id(0)\n",
        "    T = x_ref.shape[-1]\n",
        "    log2_e = 1.44269504\n",
        "    exp = lambda x: jnp.exp2(x * log2_e)\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(\n",
        "    softmax_kernel,\n",
        "    softmax_spec,\n",
        "    B={\"B0\": 1, \"B1\": 32},\n",
        "    nelem={\"N0\": N0, \"N1\": N1, \"T\": 200},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Mlgj2yN_5Ss",
      "metadata": {
        "id": "4Mlgj2yN_5Ss"
      },
      "source": [
        "## Puzzle 9: Simple FlashAttention\n",
        "\n",
        "A scalar version of FlashAttention.\n",
        "\n",
        "Uses zero programs. Block size `B0` represents `k` of length `N0`.\n",
        "Block size `B0` represents `q` of length `N0`. Block size `B0` represents `v` of length `N0`.\n",
        "Sequence length is `T`. Process it `B1 < T` elements at a time.\n",
        "\n",
        "$$z_{i} = \\sum_{j} \\text{softmax}(q_1 k_1, \\ldots, q_T k_T)_j v_{j} \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/9_simple_flash_attention.png\" /></p>\n",
        "\n",
        "This can be done in 1 loop using a similar trick from the last puzzle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g-43U3RH_5St",
      "metadata": {
        "id": "g-43U3RH_5St"
      },
      "outputs": [],
      "source": [
        "N0, T = 100, 200\n",
        "\n",
        "\n",
        "def flashatt_spec(\n",
        "    q: Float32[Array, f\"{N0}\"], k: Float32[Array, f\"{T}\"], v: Float32[Array, f\"{T}\"]\n",
        ") -> Float32[Array, \"100\"]:\n",
        "    x = q[:, None] * k[None, :]\n",
        "    x_max = jnp.max(x, -1)[..., None]\n",
        "    x = x - x_max\n",
        "    x_exp = jnp.exp(x)\n",
        "    soft = x_exp / jnp.sum(x_exp, -1)[..., None]\n",
        "    return jnp.sum(v[None, :] * soft, -1)\n",
        "\n",
        "\n",
        "def flashatt_kernel(q_ref, k_ref, v_ref, z_ref, B0: int):\n",
        "    T = k_ref.shape[-1]\n",
        "    pid_i = pl.program_id(0)\n",
        "    log2_e = 1.44269504\n",
        "    exp = lambda x: jnp.exp2(x * log2_e)\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(flashatt_kernel, flashatt_spec, B={\"B0\": 32}, nelem={\"N0\": N0, \"T\": T})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae12e14b-0da4-414c-9574-ab747b2be387",
      "metadata": {
        "id": "ae12e14b-0da4-414c-9574-ab747b2be387"
      },
      "source": [
        "## Puzzle 10: Two Dimensional Convolution\n",
        "\n",
        "A batched 2D convolution.\n",
        "\n",
        "Uses one program id axis. Block size `B0` represent the batches to process out of `N0`.\n",
        "Image `x` is size is `H` by `W` with only 1 channel, and kernel `k` is size `KH` by `KW`.\n",
        "\n",
        "$$z_{i, j, k} = \\sum_{oj, ok} k_{oj,ok} \\times x_{i,j + oj, k + ok} \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/10_two_dimensional_convolution.png\" /></p>\n",
        "\n",
        "_A comment on Pallas: We highly recommend performing the loop using\n",
        "`jax.lax.fori_loop` so that you can **avoid** the static analysis of `pl.load`\n",
        "complaining about retrieving values from beyond the bounds of `x` (which is\n",
        "required for dynamic padding). You can then safely `pl.load` values from beyond\n",
        "the bounds of `x` with a correct `mask`._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82214261-9c28-4d97-87e4-816c9cd4e6b2",
      "metadata": {
        "id": "82214261-9c28-4d97-87e4-816c9cd4e6b2"
      },
      "outputs": [],
      "source": [
        "N0, T, K = 4, 8, 4\n",
        "\n",
        "\n",
        "def conv2d_spec(\n",
        "    x: Float32[Array, f\"{N0} {T} {T}\"], k: Float32[Array, f\"{K} {K}\"]\n",
        ") -> Float32[Array, f\"{N0} {T} {T}\"]:\n",
        "    z = [[None for _ in range(x.shape[2])] for _ in range(x.shape[1])]\n",
        "    x_pad = jnp.pad(x, ((0, 0), (0, k.shape[-2]), (0, k.shape[-1])))\n",
        "    for i in range(x.shape[1]):\n",
        "        for j in range(x.shape[2]):\n",
        "            z[i][j] = jnp.sum(\n",
        "                k[None, :, :] * x_pad[:, i : i + 4, j : j + 4], axis=(-1, -2)\n",
        "            )\n",
        "    z = jnp.stack(\n",
        "        [\n",
        "            jnp.stack([z[i][j] for i in range(x.shape[1])], -1)\n",
        "            for j in range(x.shape[2])\n",
        "        ],\n",
        "        -1,\n",
        "    )\n",
        "    return z\n",
        "\n",
        "\n",
        "def conv2d_kernel(x_ref, k_ref, z_ref, B0: int):\n",
        "    pid_i = pl.program_id(0)\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(\n",
        "    conv2d_kernel,\n",
        "    conv2d_spec,\n",
        "    B={\"B0\": 2},\n",
        "    nelem={\"N0\": N0, \"H\": T, \"W\": T, \"KH\": K, \"KW\": K},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2752542-0161-48e2-9efc-f7a730ebc2a6",
      "metadata": {
        "id": "d2752542-0161-48e2-9efc-f7a730ebc2a6"
      },
      "source": [
        "## Puzzle 11: Matrix Multiplication\n",
        "\n",
        "A blocked matrix multiplication.\n",
        "\n",
        "Uses three program id axes. Block size `B2` represent the batches to process out of `N2`.\n",
        "Block size `B0` represent the rows of `x` to process out of `N0`. Block size `B1` represent the cols of `y` to process out of `N1`. The middle shape is `MID`.\n",
        "\n",
        "$$z_{i, j, k} = \\sum_{l} x_{i,j, l} \\times y_{i, l, k} \\text{ for } i = 1\\ldots N_2, j = 1\\ldots N_0, k = 1\\ldots N_1$$\n",
        "\n",
        "You are allowed to use `pl.dot` which computes a smaller mat mul.\n",
        "\n",
        "Hint: the main trick is that you can split a matmul into smaller parts.\n",
        "\n",
        "$$z_{i, j, k} = \\sum_{l=1}^{L/2} x_{i,j, l} \\times y_{i, l, k} +  \\sum_{l=L/2}^{L} x_{i,j, l} \\times y_{i, l, k} $$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/11_matrix_multiplication.png\" /></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111c598f-e402-400b-9b44-5514198fd9f3",
      "metadata": {
        "id": "111c598f-e402-400b-9b44-5514198fd9f3"
      },
      "outputs": [],
      "source": [
        "N0, N1, N2, MID = 32, 32, 4, 64\n",
        "\n",
        "\n",
        "def dot_spec(\n",
        "    x: Float32[Array, f\"{N2} {N0} {MID}\"], y: Float32[Array, f\"{N2} {MID} {N1}\"]\n",
        ") -> Float32[Array, f\"{N2} {N0} {N1}\"]:\n",
        "    return x @ y\n",
        "\n",
        "\n",
        "def dot_kernel(x_ref, y_ref, z_ref, B0: int, B1: int, B2: int, B_MID: int):\n",
        "    pid_i, pid_j, pid_k = pl.program_id(0), pl.program_id(1), pl.program_id(2)\n",
        "    N2, N0, MID = x_ref.shape\n",
        "    _, MID, N1 = y_ref.shape\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "\n",
        "test(\n",
        "    dot_kernel,\n",
        "    dot_spec,\n",
        "    B={\"B0\": 16, \"B1\": 16, \"B2\": 1, \"B_MID\": 16},\n",
        "    nelem={\"N0\": N0, \"N1\": N1, \"N2\": N2, \"MID\": MID},\n",
        "    rtol=3e-3,\n",
        "    atol=3e-3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8917111-6ceb-4491-9f7a-733dd4f29a04",
      "metadata": {
        "id": "d8917111-6ceb-4491-9f7a-733dd4f29a04"
      },
      "source": [
        "## Puzzle 12: Quantized Matrix Multiplication\n",
        "\n",
        "When doing matrix multiplication with quantized neural networks a common strategy is to store the weight matrix in lower precision, with a shift and scale term.\n",
        "\n",
        "For this problem our `weight` will be stored in 4 bits. We can store `FPINT` of these in a 32 bit integer. In addition for every `group` weights in order we will store 1 `scale` float value and 1 `shift` 4 bit value. We store these for the column of weight. The `activation`s are stored separately in standard floats.\n",
        "\n",
        "Mathematically it looks like.\n",
        "\n",
        "$$z_{j, k} = \\sum_{l} sc_{j, \\frac{l}{g}} (w_{j, l} - sh_{j, \\frac{l}{g}}) \\times y_{l, k} \\text{ for } i = 1\\ldots N_2, j = 1\\ldots N_0, k = 1\\ldots N_1$$\n",
        "\n",
        "<p align=\"center\"><img alt=\"constant add illustration\" width=\"50%\" src=\"../assets/12_quantized_matrix_multiplication.png\" /></p>\n",
        "\n",
        "However, it is a bit more complex since we need to also extract the 4-bit values into floats to begin.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5699ccd-d8f1-423f-816c-2a41abc8eb19",
      "metadata": {
        "id": "e5699ccd-d8f1-423f-816c-2a41abc8eb19"
      },
      "outputs": [],
      "source": [
        "FPINT = 32 // 4\n",
        "GROUP = 8\n",
        "N0, N1, MID = 32, 32, 64\n",
        "\n",
        "\n",
        "def extract_4bit(x: Int32) -> Int32:\n",
        "    \"\"\"Unpack 4-bit integers from 32-bit integers.\"\"\"\n",
        "    over = jnp.arange(FPINT, dtype=jnp.int32) * 4\n",
        "    mask = 2**4 - 1\n",
        "    return ((x[..., None] >> over) & mask).reshape(x.shape[:-1] + (-1,))\n",
        "\n",
        "\n",
        "def broadcast_group(z: Float32 | Int32, group: int):\n",
        "    \"\"\"Broadcast each scalar to `group` elements contiguously in the last dimension.\"\"\"\n",
        "    return jnp.broadcast_to(z[..., None], z.shape + (group,)).reshape(\n",
        "        z.shape[:-1] + (-1,)\n",
        "    )\n",
        "\n",
        "\n",
        "def quant_dot_spec(\n",
        "    scale: Float32[Array, f\"{N0} {MID // GROUP}\"],\n",
        "    offset: Int32[Array, f\"{N0} {MID // FPINT // GROUP}\"],\n",
        "    weight: Int32[Array, f\"{N0} {MID // FPINT}\"],\n",
        "    activation: Float32[Array, f\"{MID} {N1}\"],\n",
        ") -> Float32[Array, f\"{N0} {N1}\"]:\n",
        "    scale = broadcast_group(scale, GROUP)\n",
        "    weight_float = scale * (\n",
        "        extract_4bit(weight) - broadcast_group(extract_4bit(offset), GROUP)\n",
        "    )\n",
        "    return weight_float @ activation\n",
        "\n",
        "\n",
        "def quant_dot_kernel(\n",
        "    scale_ref: Float32[Array, f\"{N0} {MID // GROUP}\"],\n",
        "    offset_ref: Int32[Array, f\"{N0} {MID // FPINT // GROUP}\"],\n",
        "    weight_ref: Int32[Array, f\"{N0} {MID // FPINT}\"],\n",
        "    activation_ref: Float32[Array, f\"{MID} {N1}\"],\n",
        "    z_ref: Float32[Array, f\"{N0} {N1}\"],\n",
        "    B0: int,\n",
        "    B1: int,\n",
        "    B_MID: int,\n",
        "):\n",
        "    pid_i, pid_j = pl.program_id(0), pl.program_id(1)\n",
        "    B_weight = B_MID // FPINT\n",
        "    assert B_weight * FPINT == B_MID, \"B_MID must be divisible by FPINT\"\n",
        "    B_offset = B_MID // FPINT // GROUP\n",
        "    assert B_offset * GROUP * FPINT == B_MID, \"B_MID must be divisible by FPINT * GROUP\"\n",
        "    B_scale = B_MID // GROUP\n",
        "    assert B_scale * GROUP == B_MID, \"B_MID must be divisible by GROUP\"\n",
        "    pass\n",
        "    # finish me!\n",
        "\n",
        "    row_mask = ((pid_i * B0 + jnp.arange(B0)) < weight_ref.shape[0])[:, None]\n",
        "    col_mask = ((pid_j * B1 + jnp.arange(B1)) < activation_ref.shape[1])[None, :]\n",
        "\n",
        "    def body_fn(k, acc):\n",
        "        weight_idx = (pl.dslice(B0 * pid_i, B0), pl.dslice(B_weight * k, B_weight))\n",
        "        weight_mask = (\n",
        "            row_mask\n",
        "            & (B_weight * k + jnp.arange(B_weight) < weight_ref.shape[1])[None, :]\n",
        "        )\n",
        "        weight_4bit = pl.load(weight_ref, weight_idx, mask=weight_mask)\n",
        "\n",
        "        scale_idx = (pl.dslice(B0 * pid_i, B0), pl.dslice(B_scale * k, B_scale))\n",
        "        scale_mask = (\n",
        "            row_mask & (B_scale * k + jnp.arange(B_scale) < scale_ref.shape[1])[None, :]\n",
        "        )\n",
        "        scale = pl.load(scale_ref, scale_idx, mask=scale_mask)\n",
        "\n",
        "        offset_idx = (pl.dslice(B0 * pid_i, B0), pl.dslice(B_offset * k, B_offset))\n",
        "        offset_mask = (\n",
        "            row_mask\n",
        "            & (B_offset * k + jnp.arange(B_offset) < offset_ref.shape[1])[None, :]\n",
        "        )\n",
        "        offset = pl.load(offset_ref, offset_idx, mask=offset_mask)\n",
        "\n",
        "        scale = broadcast_group(scale, GROUP)\n",
        "        offset = broadcast_group(extract_4bit(offset), GROUP)\n",
        "        weight_float = scale * (extract_4bit(weight_4bit) - offset)\n",
        "        act_idx = (pl.dslice(B_MID * k, B_MID), pl.dslice(B1 * pid_j, B1))\n",
        "        act_mask = (\n",
        "            col_mask\n",
        "            & (B_MID * k + jnp.arange(B_MID) < activation_ref.shape[0])[:, None]\n",
        "        )\n",
        "        act = pl.load(activation_ref, act_idx, mask=act_mask)\n",
        "        w_act = pl.dot(weight_float, act)\n",
        "        return acc + w_act\n",
        "\n",
        "    MID = activation_ref.shape[0]\n",
        "    acc = jnp.zeros((B0, B1), dtype=z_ref.dtype)\n",
        "    acc = jax.lax.fori_loop(0, pl.cdiv(MID, B_MID), body_fn, acc)\n",
        "    z_mask = row_mask & col_mask\n",
        "    pl.store(\n",
        "        z_ref, (pl.dslice(B0 * pid_i, B0), pl.dslice(B1 * pid_j, B1)), acc, mask=z_mask\n",
        "    )\n",
        "\n",
        "\n",
        "test(\n",
        "    quant_dot_kernel,\n",
        "    quant_dot_spec,\n",
        "    B={\"B0\": 16, \"B1\": 16, \"B_MID\": 64},\n",
        "    nelem={\"N0\": 32, \"N1\": 32, \"MID\": 64},\n",
        "    rtol=1e-2,\n",
        "    atol=1e-2,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pallas-puzzles-2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
